{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oo5HVXOUFCG"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qssjcCjIcziG"
      },
      "source": [
        "transfofrmations practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ySWEe5skcxBn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col,desc,upper,lower,sum,when\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"test\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xi7WmgtTdHj9"
      },
      "outputs": [],
      "source": [
        "df = spark.read.option(\"header\",True).option(\"inferschema\",True).csv(\"/content/sample_data/california_housing_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p84jVmM8eGHu"
      },
      "source": [
        "when otherwise // case when in sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdJmh7eAdpD0"
      },
      "outputs": [],
      "source": [
        "# when ,otherwise\n",
        "df.withColumn(\"new_col\",when(col(\"longitude\") >= -112,\"around california\").\\\n",
        "              when(col(\"longitude\") < -112,\"away from california\").otherwise(\"I dont know where you are\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wjvHxjZmxhTe"
      },
      "outputs": [],
      "source": [
        "df2 = df.withColumn(\"test\",when(col(\"longitude\") >= -119,\"around california\")\\\n",
        "              .otherwise(\"I dont know where you are\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPXIDVn6x7sd"
      },
      "outputs": [],
      "source": [
        "# pviot : aggregation function\n",
        "df2.groupBy(col(\"longitude\")).pivot(\"test\").sum(\"latitude\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnJ213zwegMD"
      },
      "outputs": [],
      "source": [
        "# changing name of a column\n",
        "df.withColumnRenamed(\"longitude\",\"long\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttdls1XynLs-"
      },
      "outputs": [],
      "source": [
        "# changing data type\n",
        "# using cast()\n",
        "\n",
        "df.withColumn(\"longitude\",col(\"longitude\").cast(\"float\")).printSchema()\n",
        "# OR\n",
        "df.select(col(\"longitude\").cast(\"float\")).printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCk_Lnv-wDiR"
      },
      "source": [
        "sort and orderby sre same in Pyspark dataframe\n",
        "\n",
        "but in sql sort by and orderby are diff.\n",
        "\n",
        "sortby sorts within partitons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUwsx-Xhg4JS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# sorting\n",
        "df.sort(col(\"longitude\").desc()).show()\n",
        "\n",
        "df.orderBy(col(\"longitude\").desc()).show()\n",
        "# orderBy\n",
        "df.select(col(\"longitude\"),col(\"housing_median_age\")).orderBy(col(\"housing_median_age\").desc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW8CS3j1jkEP"
      },
      "outputs": [],
      "source": [
        "# # like\n",
        "# df.where(col(\"longitude\").like(\"-1%\")).show()\n",
        "# df.filter(col(\"longitude\").like(\"-1%\")).show()\n",
        "df.filter(col(\"latitude\").like(\"3%\")).select(\"longitude\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHTnsOzB573B"
      },
      "outputs": [],
      "source": [
        "# count number of records\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwcd0-HD5_GH"
      },
      "outputs": [],
      "source": [
        "# distinct\n",
        "df2 = df.dropDuplicates([\"longitude\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6abgBBj6O6e"
      },
      "outputs": [],
      "source": [
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhadEqEU6QR2"
      },
      "outputs": [],
      "source": [
        "l=[\"sort and orderby sre same\", \"in Pyspark dataframe but in sql\",\\\n",
        "  \" sort by and orderby are diff. sortby sorts within partitons\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZpt4e95Srws"
      },
      "outputs": [],
      "source": [
        "r = spark.sparkContext.parallelize(l,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LozuLbu4Sz_k"
      },
      "outputs": [],
      "source": [
        "r.glom().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL8CmVbGS5Zs"
      },
      "outputs": [],
      "source": [
        "rf = r.filter(lambda x:x!=\" \")\n",
        "rf.glom().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKdJgLWATeVv"
      },
      "outputs": [],
      "source": [
        "rep=rf.repartition(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M7Dz0ufTgDk"
      },
      "outputs": [],
      "source": [
        "rep.glom().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTMPxAOcUNxb"
      },
      "outputs": [],
      "source": [
        "# reduceByKey\n",
        "df.select(col(\"longitude\")).rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq5f-Usfb7vG"
      },
      "outputs": [],
      "source": [
        "# fill and fillna\n",
        "df3 = spark.createDataFrame([[\"hello\",\"jk\"],[\"world\",\"lk\"],[\"from\",\"sk\"],[None,\"sd\"],[\"moon\",None]],schema=\"name string,pet_name string\")\n",
        "df3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhnRJc-MJbCz"
      },
      "outputs": [],
      "source": [
        "df3.na.fill({\"name\":\"not given\",\"pet_name\":\"NA\"}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFqDg62HMQuR"
      },
      "outputs": [],
      "source": [
        "# sample\n",
        "df3.sample(0.6).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ksoRvKMqO6d3"
      },
      "outputs": [],
      "source": [
        "# UDF : user defined function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9Dm0pPHYH0H"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType,IntegerType\n",
        "def total_pay(a,b)->int:\n",
        "  return int(a+b)\n",
        "totalPay = udf(lambda x,y : total_pay(x,y),IntegerType())\n",
        "df.select(totalPay(col(\"longitude\"),col(\"latitude\")).alias(\"location\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzTduOPybs8F"
      },
      "outputs": [],
      "source": [
        "conf = df.rdd.context.getConf()\n",
        "# df.rdd.context.appName\n",
        "conf.set(\"spark.ui.showConsoleProgress\",False)\n",
        "conf.getAll()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TApi0QgNJum"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
